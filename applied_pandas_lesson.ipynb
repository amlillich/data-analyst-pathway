{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applied pandas: Solving the Missing Data Problem\n",
        "\n",
        "Welcome to the **Core Skills application** at **DataMart!** You've mastered the fundamentals of Python and pandas manipulation. Now, we face the most common real-world problem: messy data. Before any analysis can begin, we must ensure our sales records are accurate and complete. In this lesson, we showcase the power of the **pandas** library to perform essential data cleaning.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this 20-minute activity, you will be able to:\n",
        "\n",
        "1.  **Identify** null (missing) values in a large dataset using pandas aggregation methods.\n",
        "2.  **Apply** the correct contextual strategy (**deletion** vs. **imputation**) using pandas functions (`dropna` and `fillna`).\n",
        "3.  **Independently determine and implement** an imputation strategy (mode imputation) for categorical data.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AHLVnqbZKwyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Import Libraries and Load the DataMart CSV\n",
        "\n",
        "First, we need the powerful **pandas** library for data manipulation.\n",
        "\n",
        "The data file (`messy_sales_data.csv`) is included in this repository. Since it's in the same directory as this notebook, we can load it directly using a simple file name.\n",
        "\n",
        "**Action:** Run the code cell below to import the library and load the data into a DataFrame named `df`."
      ],
      "metadata": {
        "id": "5iNlapcaKWDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data directly from the root folder\n",
        "file_name = 'messy_sales_data.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "print(f\"Data successfully loaded from {file_name}!\")"
      ],
      "metadata": {
        "id": "J9kL4k-ZKXL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Identify the Missing Data\n",
        "\n",
        "Before we can clean anything, we need to know *where* the problems are.\n",
        "\n",
        "The quickest way to check for missing values is to combine:\n",
        "\n",
        "- `.isnull()` â€” identifies nulls  \n",
        "- `.sum()` â€” counts them  \n",
        "\n",
        "**Action:** Run the cell below to inspect missing values across the dataset."
      ],
      "metadata": {
        "id": "PNCmkvueKcy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many missing values exist in each column\n",
        "print(\"--- Initial Missing Value Count ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Initial DataFrame Preview ---\")\n",
        "print(df.head(8))"
      ],
      "metadata": {
        "id": "VLmwiFliKfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Resolving Missing Customer IDs\n",
        "\n",
        "The `Customer_ID` is a unique identifier. If we can't tie a sale to a customer ID, that row is fundamentally flawed for customer-level analysis (like calculating Lifetime Value).\n",
        "\n",
        "The best practice is to **delete** any row where the `Customer_ID` is null, as the data is irreparably missing. We use the **pandas method** `.dropna()` and specify the subset of columns we want to check.\n",
        "\n",
        "**Action:** Run the code below to remove rows missing a `Customer_ID`."
      ],
      "metadata": {
        "id": "pNvox4R_LIxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows where Customer_ID is null\n",
        "# Note: We reassign to df so the changes persist\n",
        "df = df.dropna(subset=['Customer_ID'])\n",
        "\n",
        "print(\"--- After Deletion: Remaining Nulls Check ---\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nNew total number of rows: {len(df)}\")"
      ],
      "metadata": {
        "id": "A-ArwIA54NPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Resolving Missing Revenue\n",
        "\n",
        "The `Revenue` column contains quantitative financial data. In sales tracking, a missing amount usually indicates an unrecorded or zero-dollar transaction.\n",
        "\n",
        "We will use the **pandas method** `.fillna()` to replace the missing values with **0.00**.\n",
        "\n",
        "**Action:** Run the code below to handle the Revenue column."
      ],
      "metadata": {
        "id": "03ho3WCrKjQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Revenue'] = df['Revenue'].fillna(0.00)\n",
        "\n",
        "print(\"Revenue nulls resolved with 0.\")"
      ],
      "metadata": {
        "id": "INBIobfNLVD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Coding Challenge: Missing Item Categories\n",
        "\n",
        "The `Item_Category` is a *categorical* field. We can't use 0, but deleting the remaining rows would lose good sales data!\n",
        "\n",
        "**Challenge:** Fill the remaining missing values in the `Item_Category` column with the **most frequent category** in the entire column (this is called **mode imputation**).\n",
        "\n",
        "**Your task - complete these two steps:**\n",
        "1. Find the most frequent category using `.mode()[0]` on the Item_Category column\n",
        "2. Fill the missing values using `.fillna()` with that mode value\n",
        "\n",
        "**Expected outcome:** After your code runs, `Item_Category` should have 0 null values.\n",
        "\n",
        "***\n",
        "**ðŸ’¡ Hint:** The `.mode()` function returns a pandas Series (even if there is only one mode). You will need to select the first element of that Series (using `[0]`) to get the single value needed for `.fillna()`.\n",
        "***"
      ],
      "metadata": {
        "id": "qIMLd3PLLY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: Find the mode of the 'Item_Category' column and use it to fill the missing values.\n",
        "\n",
        "# ----------------- YOUR CODE HERE -----------------\n",
        "\n",
        "# --------------------------------------------------\n",
        "\n",
        "# [SOLUTION CHECK] Check your null counts after running your code:\n",
        "print(\"\\n--- After Challenge: Null Check ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "WWj23v1_Mn5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 4.2 How the Correct Solution Works: Logic Check\n",
        "\n",
        "Before revealing the solution, check whether your approach follows this logic:\n",
        "\n",
        "#### Step 1: Find the Mode\n",
        "The goal is to determine the most frequent category. You use the column Series and call the **`.mode()`** method. Because `.mode()` always returns a pandas Series (even if there is only one mode), you must extract the single required value using **`[0]`**.\n",
        "\n",
        "#### Step 2: Impute Missing Values\n",
        "You then call the **`.fillna()`** method on the `Item_Category` column, passing the mode found in Step 1 as the replacement value.\n",
        "\n",
        "#### Step 3: Verify the Result\n",
        "You use explicit assignment (`df['Item_Category'] = ...`) to update the column correctly and then check your null counts to confirm success."
      ],
      "metadata": {
        "id": "oJz9JB5U5hvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution: Item Category Imputation\n",
        "\n",
        "# 1. Find the mode (the most frequent value)\n",
        "category_mode = df['Item_Category'].mode()[0]\n",
        "\n",
        "# 2. Fill the missing values with the mode using explicit assignment\n",
        "df['Item_Category'] = df['Item_Category'].fillna(category_mode)\n",
        "\n",
        "print(f\"Filled remaining missing categories with the mode: {category_mode}\")"
      ],
      "metadata": {
        "id": "YOZpQVpGM0MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Data Validation and Ready-to-Analyze Check\n",
        "\n",
        "We have successfully applied three different cleaning techniques using pandas. Now, we confirm the result and calculate our final metric."
      ],
      "metadata": {
        "id": "-Gtp8WCkLqFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Confirm the data types and non-null count\n",
        "print(\"--- Final DataFrame Information ---\")\n",
        "# The info() method shows the full non-null count for all columns\n",
        "df.info()\n",
        "\n",
        "# 2. Calculate the total sales revenue (the result of all our cleaning!)\n",
        "total_revenue = df['Revenue'].sum()\n",
        "\n",
        "print(\"\\n--- Final Business Metric ---\")\n",
        "print(f\"Total Cleaned Revenue: ${total_revenue:,.2f}\")"
      ],
      "metadata": {
        "id": "JrNJAvMgLvGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### What You Accomplished\n",
        "\n",
        "In this lesson, you:\n",
        "\n",
        "âœ… **Loaded a messy real-world dataset** into pandas from CSV format\n",
        "\n",
        "âœ… **Identified missing values** across multiple columns using `.isnull().sum()`\n",
        "\n",
        "âœ… **Made strategic cleaning decisions** based on business context:\n",
        "   - **Deleted** rows with missing Customer_IDs (irreparable data loss)\n",
        "   - **Imputed** missing Revenue with 0 (representing zero-dollar transactions)\n",
        "   - **Imputed** missing Item_Category with the mode (most common category)\n",
        "\n",
        "âœ… **Validated your cleaning** by confirming zero null values remain\n",
        "\n",
        "âœ… **Calculated a key business metric** - Total Revenue from cleaned data\n",
        "\n",
        "**Real-world impact:** These same pandas techniques scale to datasets with millions of rows. The workflow you just practicedâ€”identify, strategize, clean, validateâ€”is exactly what professional data analysts do daily.\n",
        "\n",
        "---\n",
        "\n",
        "### Reflection Questions\n",
        "\n",
        "Before moving on, consider:\n",
        "\n",
        "1. **Why did we delete rows with missing Customer_IDs but impute missing Revenue?** What's the business logic behind these different strategies?\n",
        "\n",
        "2. **What are the limitations of mode imputation?** When might using the \"most common category\" lead to incorrect conclusions?\n",
        "\n",
        "3. **In real analysis, what would you investigate before choosing a cleaning strategy?** (Hint: Why is the data missing in the first place?)\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps in Your DataMart Journey\n",
        "\n",
        "**What's still needed:** While our data is now free of null values, real-world datasets have other issues:\n",
        "- **Duplicate rows** - Are we counting some sales twice?\n",
        "- **Outliers** - Is that $50,000 purchase legitimate or a data entry error?\n",
        "- **Data type issues** - Are dates actually stored as dates, or as text?\n",
        "\n",
        "**Coming next in the pathway:**\n",
        "- Advanced data cleaning (duplicates, outliers, type conversions)\n",
        "- Exploratory Data Analysis using this cleaned DataMart dataset\n",
        "- Statistical analysis to answer business questions\n",
        "\n",
        "This cleaned DataFrame will serve as the foundation for all your future DataMart analysis!"
      ],
      "metadata": {
        "id": "88RJbXWEIQyl"
      }
    }
  ]
}
