# Data Analyst Skill Pathway

## Program Overview

This curriculum is designed for career-switchers with no prior technical background, built specifically to guide them to job readiness as Data Analysts. The pathway progresses through four tiers—Foundation, Core Skills, Analytical, and Professional—using a consistent DataMart e-commerce scenario to provide authentic, contextualized experience.

---

## Instructional Strategy: Design & Philosophy

Our strategy prioritizes job relevance and resilient technical skills to minimize a learner's time-to-value. The program's design choices are justified by the following principles:

| Design Principle | Strategic Justification |
| :--- | :--- |
| Controlled Scaffolding | Knowledge progresses through four distinct tiers to minimize cognitive friction. Learners master the concept before confronting the tool, ensuring deep understanding rather than rote memorization. |
| Integrated Case Study | The persistent DataMart scenario provides immediate contextual relevance. This builds portfolio-ready domain expertise alongside technical skills, ensuring every action ties back to an authentic business problem. |
| Modern Tool Prioritization | We prioritize Python (pandas) for scale, Tableau Public for stakeholder communication, and Git/GitHub for reproducible science. This stack is a direct reflection of common mid-market job requirements, ensuring our graduates are instantly hirable. |
| Bridging the Gaps | By integrating SQLite directly within the Python notebook workflow, we force learners to translate between pandas and SQL operations. This accelerates their mastery of relational logic and dual-tool proficiency, which is critical for modern data environments. |
| Future-Proofing | We include dedicated modules on AI-Assisted Analysis and Version Control. This ensures analysts understand how to responsibly leverage new tools and collaborate effectively as part of a distributed team. |

---

## Scaffolded Skill Pathway

| Tier | Skill | Short Description | Learning Objectives | Free Tools & Technology |
| :--- | :--- | :--- | :--- | :--- |
| **Foundation** | **Data Literacy Fundamentals** | Understanding data types, common data sources, how data is used in business decisions, and basic data ethics and privacy. | • Distinguish between data types and select appropriate methods of analysis. <br> • Identify ethical risks, bias, and privacy considerations. | • Google Sheets |
| **Foundation** | **Spreadsheet Proficiency** | Using spreadsheets for calculations, lookups, filtering, sorting, and pivot tables. | • Use formulas/functions (e.g., `VLOOKUP`, `SUMIF`) for calculations and lookups. <br> • Create pivot tables to summarize and cross-tabulate datasets. | • Google Sheets |
| **Foundation** | **Descriptive Statistics** | Summarizing data using central tendency, dispersion, and distribution metrics. | • Calculate and interpret statistical measures (mean, median, standard deviation). <br> • Select appropriate summaries based on distribution shape and outliers. | • Google Sheets |
| **Foundation** | **Introduction to Python Programming** | Python fundamentals: variables, types, loops, conditionals, and functions. | • Write Python programs using basic control structures (loops and conditionals). <br> • Use functions to organize and reuse code effectively. | • Python <br> • Google Colab <br> • Jupyter Notebooks |
| **Core Skills** | **Data Manipulation with Python (pandas)** | Loading, filtering, grouping, aggregating, and joining data in the powerful pandas library. | • Filter, sort, group, and select data using pandas methods. <br> • Join and merge multiple datasets to answer specific business questions. | • Python (pandas) <br> • Google Colab |
| **Core Skills** | **SQL Fundamentals** | Writing SQL queries to filter, aggregate, and join data; understanding how pandas and SQL operations relate. Taught using SQLite in Jupyter notebooks for seamless integration. | • Write SQL queries using `SELECT`, `WHERE`, `GROUP BY`, and `JOIN`. <br> • Translate between pandas operations and equivalent SQL syntax. | • SQLite <br> • Python (`sqlite3` library) <br> • Google Colab |
| **Core Skills** | **Data Cleaning with Python** | Handling missing values, duplicates, formatting issues, type mismatches, and outliers to ensure data quality. | • Identify and resolve common data quality issues (e.g., imputation, standardization). <br> • Transform raw data into analysis-ready structures. | • Python (pandas, numpy) <br> • Google Colab |
| **Core Skills** | **Data Visualization Principles** | Principles of chart selection, accessibility, color use, and avoiding misleading visuals. | • Select appropriate chart types and apply design best practices. <br> • Critically evaluate a visualization's effectiveness and identify potential misrepresentations. | • Google Sheets |
| **Core Skills** | **Data Visualization with Python** | Creating static and interactive charts using `matplotlib` and `seaborn`. | • Build common charts (bar, line, scatter) using `matplotlib`/`seaborn`. <br> • Customize charts with proper labels, scales, and formatting for clarity. | • Python (matplotlib, seaborn) <br> • Google Colab |
| **Analytical** | **Exploratory Data Analysis (EDA)** | Identifying patterns, relationships, anomalies, and hypotheses using Python's visual and statistical tools. | • Conduct a structured EDA process to surface preliminary insights. <br> • Formulate and assess hypotheses with visual and statistical evidence. | • Python (pandas, seaborn, numpy) <br> • Google Colab |
| **Analytical** | **Statistical Analysis with Python** | Hypothesis testing, correlations, distributions, and simple linear regression. | • Perform and interpret hypothesis tests (e.g., t-test). <br> • Calculate correlations and build/interpret simple linear regressions. | • Python (scipy, statsmodels) <br> • Google Colab |
| **Analytical** | **Interactive Dashboards with Tableau** | Building interactive, professional dashboards for business stakeholders with filters and linked visuals. | • Build interactive Tableau dashboards from disparate data sources. <br> • Tailor design and interactivity to specific stakeholder reporting needs. | • Tableau Public |
| **Analytical** | **AI-Assisted Data Analysis** | Using AI responsibly for debugging, documentation, planning, and accelerating query/code writing in the analytical workflow. | • Use AI tools to accelerate analysis and refine Python/SQL code. <br> • Evaluate AI output critically for errors, bias, and business context limitations. | • ChatGPT <br> • Claude |
| **Professional** | **Business Acumen & Problem Framing** | Translating ambiguous business questions into structured analytical tasks and identifying relevant Key Performance Indicators (KPIs). | • Translate vague business questions into clear, measurable analytical problems. <br> • Identify, define, and justify the appropriate KPIs for a given business scenario. | • Google Docs |
| **Professional** | **Data Storytelling & Communication** | Communicating insights and recommendations effectively and persuasively to non-technical audiences. | • Build clear, compelling analytical narratives supported by visual evidence. <br> • Present insights and recommendations in an effective, professional format. | • Google Slides <br> • Tableau Public |
| **Professional** | **Version Control & Collaboration** | Using Git and GitHub for versioning, branching, and ensuring reproducible analytical workflows. | • Use Git/GitHub for version control, branching, and merging. <br> • Organize projects with necessary documentation (README) for reproducibility. | • Git <br> • GitHub |
| **Professional** | **Capstone Portfolio Project** | End-to-end analysis demonstrating all pathway skills using the evolving DataMart dataset. | • Complete a full analysis project from problem framing to final presentation. <br> • Deliver a professional portfolio including notebook, Tableau dashboard, and executive summary. | • All pathway tools <br> • GitHub Pages / Google Sites |
